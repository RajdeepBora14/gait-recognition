# -*- coding: utf-8 -*-
"""Gait_Recognition_Modelling_(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TBR7cpchYaG_b4HoZtC1GYGh2AC_fVBy
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow pandas numpy

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if len(device_name) > 0:
    print("Found GPU at: {}".format(device_name))
else:
    device_name = "/device:CPU:0"
    print("No GPU, using {}.".format(device_name))

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping



with tf.device(device_name):
    # Load your dataset
    file_path = "/content/drive/MyDrive/Concatenated_Output_With_Labels_Normalised.csv"
    df = pd.read_csv(file_path)

    # Assuming your data is arranged such that the first column is the label, and the rest are input features
    labels = df.iloc[:, 0]
    data = df.iloc[:, 1:]

    # Convert labels to one-hot encoding
    labels_one_hot = tf.keras.utils.to_categorical(labels - 1, num_classes=62)

    # print(len(data))

    # Reshape data for convolutional layers
    data = np.array(data).reshape((len(data), 57, 1, 1))

    # Split the dataset into training and testing sets
    train_data, test_data, train_labels, test_labels = train_test_split(data, labels_one_hot, test_size=0.2, random_state=42)


    # # Data augmentation
    # datagen = ImageDataGenerator(
    #     horizontal_flip=True,
    #     preprocessing_function=lambda x: x + np.random.normal(scale=0.01, size=x.shape) if np.random.rand() < 0.3 else x
    # )

    # Model architecture using Functional API
    inputs = tf.keras.Input(shape=(57, 1))

    # Convolutional Layer 1 with Dropout
    x = layers.Conv1D(32, (3,), strides=1)(inputs)
    x = layers.PReLU()(x)
    # x = layers.Dropout(0.5)(x)  # Adjust the dropout rate as needed

    # Convolutional Layer 2 with Dropout
    x = layers.Conv1D(64, (3,), strides=1)(x)
    x = layers.PReLU()(x)
    # x = layers.Dropout(0.5)(x)  # Adjust the dropout rate as needed

    # Pooling Layer 1
    x = layers.MaxPooling1D(pool_size=2, strides=2)(x)

    # Convolutional Layer 3 with Dropout
    x = layers.Conv1D(64, (3,), strides=1)(x)
    x = layers.PReLU()(x)
    # x = layers.Dropout(0.5)(x)  # Adjust the dropout rate as needed

    # Convolutional Layer 4 with Dropout
    x = layers.Conv1D(64, (3,), strides=1)(x)
    x = layers.PReLU()(x)
    # x = layers.Dropout(0.5)(x)  # Adjust the dropout rate as needed

    # Eltwise 1
    # x = layers.Add()([x, previous_layer_output])

    # Convolutional Layer 5 with Dropout
    x = layers.Conv1D(128, (3,), strides=1)(x)
    x = layers.PReLU()(x)
    # x = layers.Dropout(0.5)(x)  # Adjust the dropout rate as needed

    # Pooling Layer 2
    x = layers.MaxPooling1D(pool_size=2, strides=2)(x)

    # Convolutional Layer 6 with Dropout
    x = layers.Conv1D(128, (3,), strides=1)(x)
    x = layers.PReLU()(x)
    # x = layers.Dropout(0.5)(x)  # Adjust the dropout rate as needed

    # Convolutional Layer 7 with Dropout
    x = layers.Conv1D(128, (3,), strides=1)(x)
    x = layers.PReLU()(x)
    # x = layers.Dropout(0.5)(x)  # Adjust the dropout rate as needed

    # Eltwise 2
    # x = layers.Add()([x, previous_layer_output])

    # Flatten
    x = layers.Flatten()(x)

    # Fully Connected Layer
    outputs = layers.Dense(512, activation='softmax')(x)

    # Create model
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    # Compile the model
    opt = Adam(learning_rate=1e-5)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

    epochs = 50

    # Define early stopping callback
    early_stopping = EarlyStopping(monitor='val_loss', patience=epochs//10, restore_best_weights=True)


    # Train the model
    # history = model.fit(datagen.flow(data,batch_size=32), epochs=40, steps_per_epoch=len(data) // 32, verbose=1)
    # Train the model on the training set
    # Train the model with early stopping
    # history = model.fit(
    #     train_data, train_labels,
    #     validation_data=(test_data, test_labels),
    #     batch_size=32, epochs=epochs,
    #     steps_per_epoch=len(train_data) // 32,
    #     verbose=1,
    #     callbacks=[early_stopping]
    # )

    train_da

# Assuming you have already trained your model and have a trained_model variable
# history = model.fit(data, labels_one_hot, batch_size=32, epochs=40000, steps_per_epoch=len(data) // 32, verbose=1)

# Evaluate the model on the entire dataset
test_loss, test_accuracy = model.evaluate(test_data, test_labels, batch_size=32)
# print(f'Test Loss: {test_loss}')
# print(f'Test Accuracy: {test_accuracy}')

print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

# Make predictions on the entire dataset
predictions = model.predict(data)

# Convert the one-hot encoded predictions back to class labels
predicted_labels = np.argmax(predictions, axis=1)

# Convert one-hot encoded true labels back to class labels
true_labels = np.argmax(labels_one_hot, axis=1)

# Calculate classification scores
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Accuracy
accuracy = accuracy_score(true_labels, predicted_labels)
print(f'Accuracy: {accuracy}')

# Precision, Recall, F1-Score
precision = precision_score(true_labels, predicted_labels, average='weighted')
recall = recall_score(true_labels, predicted_labels, average='weighted')
f1 = f1_score(true_labels, predicted_labels, average='weighted')

print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1-Score: {f1}')

# Confusion Matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)
print('Confusion Matrix:')
print(conf_matrix)